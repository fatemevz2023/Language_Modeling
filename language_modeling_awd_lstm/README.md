# AWD-LSTM with Regularization on WikiText-2

This project trains an AWD-LSTM language model on the WikiText-2 dataset, utilizing advanced regularization techniques. Initial perplexity was ~130, improved to ~90 by applying methods from the paper 'Regularizing and Optimizing LSTM Language Models'.