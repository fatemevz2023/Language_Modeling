{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uXkcYhkIxS-"
      },
      "source": [
        "#  <font color='#FFE15D'><b> Project 2: Language Modeling AWD </b></font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrFCH9hpEyip"
      },
      "source": [
        "# 🔴 **Environment Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWoVZ_MuKif0"
      },
      "source": [
        "## 🟠 Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-7NzGdb2lsH"
      },
      "outputs": [],
      "source": [
        "!pip install -q portalocker>=2.0.0\n",
        "!pip install -q torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 torchtext==0.17.2 torchmetrics==1.3.1 numpy==2.0.0 torchdata==0.11.0 tqdm==4.67.1\n",
        "!pip uninstall -q torchtune\n",
        "!pip install -q wandb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_a3OXnSeV0z"
      },
      "source": [
        "# ⚠️ **Don't forget to restart the runtime!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNJRZe4QBudV"
      },
      "source": [
        "# 🔴 **Import Libs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhlVJEkJeTsV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import tqdm\n",
        "import torchmetrics as tm\n",
        "import wandb\n",
        "\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEzYlyeqTZqQ",
        "outputId": "f400ebdd-ac8c-40c3-fa7f-4a815c49062e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.13\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DWjGTq6T8Jg",
        "outputId": "87d0975b-0150-4ab5-c249-a8c284e4510f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numpy --> 2.0.2\n",
            "torch --> 2.2.2+cu121\n",
            "torchtext --> 0.17.2+cpu\n",
            "tqdm --> 4.67.1\n"
          ]
        }
      ],
      "source": [
        "for lib in [np, torch, torchtext, tqdm]:\n",
        "  print(lib.__name__, '-->', lib.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwaY_YcgRayy"
      },
      "source": [
        "# 🔴 **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yMS7bbmRayz"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpKbTUEIRayz"
      },
      "outputs": [],
      "source": [
        "def num_trainable_params(model):\n",
        "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
        "  return nums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w6sLRLfw398"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(seed)\n",
        "      # torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "      # torch.backends.cudnn.deterministic = True\n",
        "      # torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwlVLNJXfUJw"
      },
      "source": [
        "# 🔴 **Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqPVGv0TfUKE"
      },
      "outputs": [],
      "source": [
        "seed = 8\n",
        "\n",
        "batch_size = 80\n",
        "seq_len = 70\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "num_layers = 3\n",
        "hidden_dim = 1150\n",
        "dropoute = 0.1\n",
        "dropouti = 0.65\n",
        "dropouth = 0.3\n",
        "dropouto = 0.4\n",
        "weight_drop = 0.\n",
        "\n",
        "lr = 30\n",
        "wd = 1.2e-6\n",
        "momentum = 0.9\n",
        "\n",
        "clip = 0.25\n",
        "\n",
        "wandb_enable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXOG_oXwrICX",
        "outputId": "b0b27275-bf13-420b-efc2-aa9a55e9470d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please input the WandB argument (run) name:Base\n"
          ]
        }
      ],
      "source": [
        "wandb_arg_name = input('Please input the WandB argument (run) name:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vp5AUcSs-drV",
        "outputId": "91b45a50-a7f6-47bb-ce65-5e7241c9349c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Base'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb_arg_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTql4Ftiunfr"
      },
      "source": [
        "# 🔴 **Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujIVtjsYvxOI"
      },
      "source": [
        "## 🟠 Load the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek9DpCNCChzF"
      },
      "source": [
        "🔰 In this session you should load WikiText2 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXnDjbRVcnh7",
        "outputId": "19655bde-ecf2-4dca-d14b-4c2616557f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ0_jak84r7",
        "outputId": "827de700-0e08-41cb-f527-b8b171495a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Datasets/wikitext-2-v1.zip\n",
            "   creating: /content/wikitext-2/\n",
            "  inflating: /content/wikitext-2/wiki.test.tokens  \n",
            "  inflating: /content/wikitext-2/wiki.valid.tokens  \n",
            "  inflating: /content/wikitext-2/wiki.train.tokens  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/drive/MyDrive/Datasets/wikitext-2-v1.zip' -d '/content/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCi-ofSLCzop"
      },
      "source": [
        "## 🟠 Build vocabulary and save it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L02PHFuyNRb3"
      },
      "source": [
        "🔰 In this section we need to:\n",
        "\n",
        "*   Define a tokenizer using `basic_english`\n",
        "*   Tokenize the dataset and collect tokens\n",
        "*   Build the vocabulary using `build_vocab_from_iterator`\n",
        "*   Manually insert special tokens and set the default index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqTZoJvg-UvK"
      },
      "outputs": [],
      "source": [
        "def load_data_iterators(base_path):\n",
        "    import os\n",
        "    def read_file_gen(filename):\n",
        "        filepath = os.path.join(base_path, filename)\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            yield from (line.strip() for line in f if line.strip() and not line.startswith('='))\n",
        "\n",
        "    train_iter = read_file_gen('wiki.train.tokens')\n",
        "    valid_iter = read_file_gen('wiki.valid.tokens')\n",
        "    test_iter  = read_file_gen('wiki.test.tokens')\n",
        "\n",
        "    return train_iter, valid_iter, test_iter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EAFgri7-X7b"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/wikitext-2'\n",
        "train_iter, valid_iter, test_iter = load_data_iterators(base_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mta7AoPki4wx"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "torch.save(vocab, 'vocab.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idRexFij4wgN"
      },
      "source": [
        "## 🟠 Transform the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VjvBOtvHu2v"
      },
      "source": [
        "🛑 Make sure to perform the transformations on train, validation and test datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApisIcGeGSsJ"
      },
      "source": [
        "🔰 Reshape the dataset into an `N x B x L` or `M x L` format, where `N` represents the number of batches, `B` is the batch size, `L` is the length of a sample within each batch, and `M` is equal to `N x B`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocxM8YdsWH-1"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_text_iter, seq_len):\n",
        "  data = torch.cat([torch.LongTensor(vocab(tokenizer(line))) for line in raw_text_iter])\n",
        "\n",
        "  M = len(data) // seq_len\n",
        "\n",
        "  r = len(data) % seq_len\n",
        "  data = torch.cat((data, torch.LongTensor([0]))) if r==0 else data\n",
        "\n",
        "  inputs = data[:M*seq_len]\n",
        "  targets = data[1:M*seq_len+1]\n",
        "\n",
        "  inputs = inputs.reshape(-1, seq_len)\n",
        "  targets = targets.reshape(-1, seq_len)\n",
        "\n",
        "  return inputs, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Am95DPRUOo",
        "outputId": "7292d885-87e8-45e6-9453-56b77637239d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([29285, 70]),\n",
              " torch.Size([29285, 70]),\n",
              " torch.Size([3063, 70]),\n",
              " torch.Size([3063, 70]),\n",
              " torch.Size([3455, 70]),\n",
              " torch.Size([3455, 70]))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, y_train = data_process(train_iter, seq_len)\n",
        "X_valid, y_valid = data_process(valid_iter, seq_len)\n",
        "X_test, y_test = data_process(test_iter, seq_len)\n",
        "\n",
        "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgLgP04P4-aX"
      },
      "source": [
        "## 🟠 Custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkxH_IR2PBNq"
      },
      "source": [
        "🔰 Write a custom dataset class for LanguageModelDataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cjpSkrtexap"
      },
      "outputs": [],
      "source": [
        "class LanguageModelDataset(Dataset):\n",
        "\n",
        "  def __init__(self, inputs, targets):\n",
        "    self.inputs = inputs\n",
        "    self.targets = targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.inputs.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.inputs[idx], self.targets[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0qUkL0CfQmr"
      },
      "outputs": [],
      "source": [
        "train_set = LanguageModelDataset(X_train, y_train)\n",
        "valid_set = LanguageModelDataset(X_valid, y_valid)\n",
        "test_set = LanguageModelDataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92IiPxSMSPdz",
        "outputId": "16eaeee2-f793-4ba6-ec16-e7b5217cf7fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([    9,  3849,  3869,   881,     9, 20000,    83,  3849,    88,     0,\n",
              "          3869,    21,   780, 28780,     2,  6182,     3,  3849,     4,     1,\n",
              "          5023,    88,    20,     2,  1837,  1018,     7,    14,  3849,  3869,\n",
              "           881,   629,   976,     2,    23,     8,  5790,   299,    12,   575,\n",
              "           232,    67,   452,    19, 13722,     5,   757,     3,  2500,    17,\n",
              "             1,  1767,  5637,     3,   155,     6,   246,   354,     6,   976,\n",
              "             2,    24,    23,     1,   237,    67,     6,     1,  3849,    93]),\n",
              " tensor([ 3849,  3869,   881,     9, 20000,    83,  3849,    88,     0,  3869,\n",
              "            21,   780, 28780,     2,  6182,     3,  3849,     4,     1,  5023,\n",
              "            88,    20,     2,  1837,  1018,     7,    14,  3849,  3869,   881,\n",
              "           629,   976,     2,    23,     8,  5790,   299,    12,   575,   232,\n",
              "            67,   452,    19, 13722,     5,   757,     3,  2500,    17,     1,\n",
              "          1767,  5637,     3,   155,     6,   246,   354,     6,   976,     2,\n",
              "            24,    23,     1,   237,    67,     6,     1,  3849,    93,     3]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCQjacybOfqV"
      },
      "source": [
        "## 🟠 Define a dataloader if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqKMEyFNS-1a"
      },
      "source": [
        "🔰 Write dataloaders for the training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMCJ3UMD0U_f"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lRY0FnUTDEN",
        "outputId": "649c7728-69e3-45f2-d34d-68c046eef029"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([80, 70]),\n",
              " torch.Size([80, 70]),\n",
              " tensor([[ 1985,    13,     1,  ...,  1985,    13,     1],\n",
              "         [  104,     2,    57,  ..., 16138,  2285,    92],\n",
              "         [    2,    22,   100,  ...,   116,    22,     2],\n",
              "         ...,\n",
              "         [   22,     0,   173,  ...,    37, 12908,     6],\n",
              "         [    6,    43,  8400,  ...,    93,     3,     1],\n",
              "         [25828,    65,    46,  ...,     3,   179,  1108]]))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_batch, y_batch = next(iter(train_loader))\n",
        "x_batch.shape, y_batch.shape, x_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVoUEQm1yhNi",
        "outputId": "d89a9c5c-b2b4-4062-a3f9-6fc4916ef041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1985) tensor(13)\n"
          ]
        }
      ],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "for inputs, targets in train_loader:\n",
        "  print(inputs[0, 0], targets[0, 0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ttl0AK3Hvyh"
      },
      "source": [
        "# 🔴 **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWDUTrsIzRhr"
      },
      "outputs": [],
      "source": [
        "class WeightDrop(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, module, weights, dropout=0):\n",
        "    super(WeightDrop, self).__init__()\n",
        "    self.module = module\n",
        "    self.weights = weights\n",
        "    self.dropout = dropout\n",
        "    self._setup()\n",
        "\n",
        "  def widget_demagnetizer_y2k_edition(*args, **kwargs):\n",
        "    return\n",
        "\n",
        "  def _setup(self):\n",
        "    if issubclass(type(self.module), torch.nn.RNNBase):\n",
        "      self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n",
        "\n",
        "      for name_w in self.weights:\n",
        "        print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n",
        "        w = getattr(self.module, name_w)\n",
        "        del self.module._parameters[name_w]\n",
        "        self.module.register_parameter(name_w + '_raw', nn.Parameter(w.data))\n",
        "\n",
        "  def _setweights(self):\n",
        "    for name_w in self.weights:\n",
        "      raw_w = getattr(self.module, name_w + '_raw')\n",
        "      w = None\n",
        "      # w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n",
        "      mask = torch.nn.functional.dropout(torch.ones_like(raw_w), p=self.dropout, training=True) * (1 - self.dropout)\n",
        "      setattr(self.module, name_w, raw_w * mask)\n",
        "\n",
        "  def forward(self, *args):\n",
        "    self._setweights()\n",
        "    return self.module.forward(*args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2rOXE6mNe68"
      },
      "outputs": [],
      "source": [
        "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
        "  if dropout:\n",
        "    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(\n",
        "        embed.weight) / (1 - dropout)\n",
        "    masked_embed_weight = mask * embed.weight\n",
        "  else:\n",
        "    masked_embed_weight = embed.weight\n",
        "  if scale:\n",
        "    masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
        "\n",
        "  padding_idx = embed.padding_idx\n",
        "  if padding_idx is None:\n",
        "    padding_idx = -1\n",
        "\n",
        "  embedding = torch.nn.functional.embedding(words, masked_embed_weight,\n",
        "                                            padding_idx, embed.max_norm, embed.norm_type,\n",
        "                                            embed.scale_grad_by_freq, embed.sparse)\n",
        "  return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwBQ3I-XLIHw"
      },
      "outputs": [],
      "source": [
        "class LockedDropout(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LockedDropout, self).__init__()\n",
        "\n",
        "  def forward(self, x, dropout):\n",
        "    if not self.training or not dropout:\n",
        "      return x\n",
        "    m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "    mask = m.requires_grad_(False) / (1 - dropout)\n",
        "    mask = mask.expand_as(x)\n",
        "    return mask * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baOLnaB8jVC-"
      },
      "source": [
        "🔰 AWD-LSTM Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISnnHE0BMVqp"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers,\n",
        "               dropoute=0.2, dropouti=0.2, dropouth=0.2, dropouto=0.2,\n",
        "               weight_drop=0.2):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    self.lstms = []\n",
        "    self.lstms.append(nn.LSTM(embedding_dim, hidden_dim, num_layers=1, dropout=0, batch_first=False))\n",
        "    self.lstms.append(nn.LSTM(hidden_dim, hidden_dim, num_layers=1, dropout=0, batch_first=False))\n",
        "    self.lstms.append(nn.LSTM(hidden_dim, embedding_dim, num_layers=1, dropout=0, batch_first=False))\n",
        "    if weight_drop > 0:\n",
        "      self.lstms = [WeightDrop(lstm, ['weight_hh_l0'], dropout=weight_drop) for lstm in self.lstms]\n",
        "    self.lstms = nn.ModuleList(self.lstms)\n",
        "\n",
        "    self.fc = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    self.fc.weight = self.embedding.weight\n",
        "\n",
        "    self.lockdrop = LockedDropout()\n",
        "    self.dropoute = dropoute\n",
        "    self.dropouti = dropouti\n",
        "    self.dropouth = dropouth\n",
        "    self.dropouto = dropouto\n",
        "    # print(dropoute, dropouti, dropouth, dropouto)\n",
        "\n",
        "  def forward(self, src):\n",
        "    embedding = embedded_dropout(self.embedding, src, dropout=self.dropoute if self.training else 0)\n",
        "    embedding = self.lockdrop(embedding, self.dropouti)\n",
        "\n",
        "    new_hiddens = []\n",
        "    for l, lstm in enumerate(self.lstms):\n",
        "      embedding, _ = lstm(embedding)\n",
        "      if l != self.num_layers-1:\n",
        "        embedding = self.lockdrop(embedding, self.dropouth)\n",
        "\n",
        "    embedding = self.lockdrop(embedding, self.dropouto)\n",
        "\n",
        "    prediction = self.fc(embedding)\n",
        "    return prediction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MgBVzorb9oQ",
        "outputId": "0441ca32-1f91-4893-ed8c-145b7269e43e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (embedding): Embedding(28782, 300)\n",
              "  (lstms): ModuleList(\n",
              "    (0): LSTM(300, 1150)\n",
              "    (1): LSTM(1150, 1150)\n",
              "    (2): LSTM(1150, 300)\n",
              "  )\n",
              "  (fc): Linear(in_features=300, out_features=28782, bias=True)\n",
              "  (lockdrop): LockedDropout()\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "model = LanguageModel(vocab_size=len(vocab), embedding_dim=embedding_dim,\n",
        "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
        "                      dropoute=dropoute, dropouti=dropouti,\n",
        "                      dropouth=dropouth, dropouto=dropouto,\n",
        "                      weight_drop=weight_drop)\n",
        "model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUdG6zVt0AJs"
      },
      "outputs": [],
      "source": [
        "type(model.lstms[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOPLTufglrsy",
        "outputId": "1734d82a-dd00-471e-89d6-a21ba6b84a5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[-0.0269, -0.0171,  0.0099,  ...,  0.0265,  0.0148,  0.0252],\n",
              "         [-0.0029, -0.0240,  0.0127,  ...,  0.0204, -0.0181,  0.0231],\n",
              "         [ 0.0243,  0.0071,  0.0120,  ..., -0.0033,  0.0135,  0.0114],\n",
              "         ...,\n",
              "         [-0.0184, -0.0187, -0.0229,  ...,  0.0111,  0.0260, -0.0071],\n",
              "         [ 0.0007, -0.0156, -0.0018,  ..., -0.0201, -0.0130, -0.0003],\n",
              "         [-0.0117,  0.0278,  0.0266,  ...,  0.0072,  0.0089, -0.0032]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.0056, -0.0289, -0.0022,  ...,  0.0155,  0.0037,  0.0013],\n",
              "         [-0.0135,  0.0206,  0.0283,  ...,  0.0168,  0.0229,  0.0231],\n",
              "         [ 0.0090, -0.0099,  0.0035,  ...,  0.0231, -0.0041,  0.0109],\n",
              "         ...,\n",
              "         [-0.0029,  0.0111, -0.0024,  ...,  0.0149, -0.0293, -0.0250],\n",
              "         [-0.0160,  0.0080,  0.0199,  ..., -0.0099,  0.0070,  0.0180],\n",
              "         [ 0.0277, -0.0171, -0.0076,  ...,  0.0268, -0.0164, -0.0243]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.0743,  0.0229,  0.0152,  ..., -0.0480, -0.0062, -0.0140],\n",
              "         [ 0.0521, -0.0448,  0.0947,  ..., -0.0714, -0.0514,  0.0090],\n",
              "         [ 0.0643,  0.0408, -0.0952,  ...,  0.0231,  0.0296, -0.0876],\n",
              "         ...,\n",
              "         [ 0.0932, -0.0317, -0.0590,  ..., -0.0091, -0.0752, -0.0565],\n",
              "         [-0.0761,  0.0487,  0.0138,  ..., -0.0595, -0.0196,  0.0560],\n",
              "         [ 0.0822, -0.0755,  0.0345,  ..., -0.0358, -0.0843, -0.0540]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.0743,  0.0229,  0.0152,  ..., -0.0480, -0.0062, -0.0140],\n",
              "         [ 0.0521, -0.0448,  0.0947,  ..., -0.0714, -0.0514,  0.0090],\n",
              "         [ 0.0643,  0.0408, -0.0952,  ...,  0.0231,  0.0296, -0.0876],\n",
              "         ...,\n",
              "         [ 0.0932, -0.0317, -0.0590,  ..., -0.0091, -0.0752, -0.0565],\n",
              "         [-0.0761,  0.0487,  0.0138,  ..., -0.0595, -0.0196,  0.0560],\n",
              "         [ 0.0822, -0.0755,  0.0345,  ..., -0.0358, -0.0843, -0.0540]],\n",
              "        requires_grad=True))"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.lstms[0].weight_hh_l0, model.lstms[0].weight_ih_l0, model.embedding.weight, model.fc.weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOpzY8sxbWYH",
        "outputId": "8d9849b8-fc4c-4375-8f9b-927f96670c4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([70, 80, 28782]), torch.Size([80, 70]))"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(x_batch.t()).shape, x_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xNyCwU8bKQ6",
        "outputId": "e6d508cd-246e-4df4-a4db-4e55483f719e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27.674182"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_trainable_params(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1GGav-WRoRYe",
        "outputId": "5d5a9f1f-7d0e-418b-f93f-dffa34dcd709"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8.6346"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_trainable_params(model.embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcNCBiTo0slj"
      },
      "outputs": [],
      "source": [
        "data_np = model.embedding.weight.cpu().detach().numpy()\n",
        "unique_rows, indices, counts = np.unique(data_np, axis=0, return_index=True, return_counts=True)\n",
        "len(unique_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Mnh06_1sdO"
      },
      "outputs": [],
      "source": [
        "glove = GloVe(name='6B', dim=glove_dim)\n",
        "glove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24qT-sgUO2-d"
      },
      "source": [
        "# 🔴 **Config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma28M5Z36gsq",
        "outputId": "ab78cebb-295f-4384-f0c8-4c32aba79246"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ubk3xKaIG6i"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "metric = tm.text.Perplexity().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5znK6USrlVd5",
        "outputId": "9fa188bd-613c-4008-b89e-c280890aa001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key file does not exist. Please create the key file with your wandb API key.\n"
          ]
        }
      ],
      "source": [
        "key_file = '/content/key'\n",
        "\n",
        "if os.path.exists(key_file):\n",
        "    with open(key_file) as f:\n",
        "        key = f.readline().strip()\n",
        "    wandb.login(key=key)\n",
        "else:\n",
        "    print(\"Key file does not exist. Please create the key file with your wandb API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0QNbC0YPCKZ"
      },
      "source": [
        "# 🔴 **Train ➰**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS6EF4HUhi5e"
      },
      "source": [
        "🔰 This is the template for train function, change it if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WniOAgk0QyRI"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, loss_fn, optimizer, metric, epoch=None):\n",
        "  model.train()\n",
        "  loss_train = AverageMeter()\n",
        "  metric.reset()\n",
        "\n",
        "  with tqdm.tqdm(train_loader, unit='batch') as tepoch:\n",
        "    for inputs, targets in tepoch:\n",
        "      if epoch:\n",
        "        tepoch.set_description(f'Epoch {epoch}')\n",
        "\n",
        "      inputs = inputs.t().to(device)\n",
        "      targets = targets.t().to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      loss = loss_fn(outputs.reshape(-1, outputs.shape[-1]), targets.flatten())\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      nn.utils.clip_grad.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss_train.update(loss.item(), n=len(targets))\n",
        "      metric.update(outputs, targets)\n",
        "\n",
        "      tepoch.set_postfix(loss=loss_train.avg, metric=metric.compute().item())\n",
        "\n",
        "  return model, loss_train.avg, metric.compute().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9HgVWslPGsH"
      },
      "source": [
        "# 🔴 **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsszJ7GVj2l3"
      },
      "source": [
        "🔰 This is the template for evaluation function, change it if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV0_67_ZQ0xf"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, loss_fn, metric):\n",
        "  model.eval()\n",
        "  loss_eval = AverageMeter()\n",
        "  metric.reset()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for inputs, targets in test_loader:\n",
        "      inputs = inputs.t().to(device)\n",
        "      targets = targets.t().to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      loss = loss_fn(outputs.reshape(-1, outputs.shape[-1]), targets.flatten())\n",
        "      loss_eval.update(loss.item(), n=len(targets))\n",
        "\n",
        "      metric(outputs, targets)\n",
        "\n",
        "  return loss_eval.avg, metric.compute().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_5f69nwPtY2"
      },
      "source": [
        "# 🔴 **Training Process 〽️**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7VreNxQdct"
      },
      "source": [
        "## 🟠 Finding Hyper-parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpJ3wtyctQJH"
      },
      "source": [
        "### 🟡 **Step 1:** Calculate the loss for an untrained model using a few batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnE4F4GkzzaR"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(len(vocab), embedding_dim=300,\n",
        "                      hidden_dim=512, num_layers=2,\n",
        "                      dropout_embd=0.5, dropout_rnn=0.2).to(device)\n",
        "\n",
        "inputs, targets = next(iter(train_loader))\n",
        "inputs = inputs.to(device)\n",
        "targets = targets.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(inputs)\n",
        "  loss = loss_fn(outputs.reshape(-1, outputs.shape[-1]), targets.flatten())\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxHX7YRCBVxM"
      },
      "outputs": [],
      "source": [
        "outputs.reshape(-1, outputs.shape[-1]).shape, targets.flatten().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHj-Mp8FA_DW"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrHQCv7q7LF_"
      },
      "source": [
        "### 🟡 **Step 2:** Try to train and overfit the model on a small subset of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0ji0MXsWaPt"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(len(vocab), embedding_dim=embedding_dim,\n",
        "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
        "                      dropout_embd=dropout_embd, dropout_rnn=dropout_rnn).to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.9, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRZQpPWJ2qv"
      },
      "outputs": [],
      "source": [
        "mini_train_size = 1000\n",
        "_, mini_train_dataset = random_split(train_set, (len(train_set)-mini_train_size, mini_train_size))\n",
        "mini_train_loader = DataLoader(mini_train_dataset, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6FNIFCM6A6al",
        "outputId": "17f464a7-833d-45f8-f6eb-79ae0d2767c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 16px;\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [05:12<00:00,  6.24s/batch, loss=8.27, metric=3.9e+3]\n",
            "Epoch 1: 100%|██████████| 50/50 [05:00<00:00,  6.01s/batch, loss=7.2, metric=1.34e+3]\n",
            "Epoch 2: 100%|██████████| 50/50 [04:54<00:00,  5.90s/batch, loss=7.04, metric=1.14e+3]\n",
            "Epoch 3: 100%|██████████| 50/50 [04:54<00:00,  5.90s/batch, loss=6.98, metric=1.08e+3]\n",
            "Epoch 4: 100%|██████████| 50/50 [04:54<00:00,  5.89s/batch, loss=6.95, metric=1.04e+3]\n",
            "Epoch 5: 100%|██████████| 50/50 [04:52<00:00,  5.85s/batch, loss=6.92, metric=1.01e+3]\n",
            "Epoch 6: 100%|██████████| 50/50 [04:54<00:00,  5.89s/batch, loss=6.86, metric=957]\n",
            "Epoch 7: 100%|██████████| 50/50 [04:56<00:00,  5.93s/batch, loss=6.79, metric=885]\n",
            "Epoch 8: 100%|██████████| 50/50 [04:54<00:00,  5.89s/batch, loss=6.71, metric=817]\n",
            "Epoch 9: 100%|██████████| 50/50 [04:55<00:00,  5.91s/batch, loss=6.64, metric=761]\n",
            "Epoch 10: 100%|██████████| 50/50 [04:55<00:00,  5.90s/batch, loss=6.58, metric=722]\n",
            "Epoch 11: 100%|██████████| 50/50 [04:53<00:00,  5.87s/batch, loss=6.49, metric=662]\n",
            "Epoch 12:  14%|█▍        | 7/50 [00:40<04:04,  5.69s/batch, loss=6.44, metric=626]"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  model, _, _ = train_one_epoch(model, mini_train_loader, loss_fn, optimizer, metric, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLT4w0ZfAhlJ"
      },
      "source": [
        "### 🟡 **Step 3:** Train the model for a limited number of epochs, experimenting with various learning rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AqFqm1smGVrX",
        "outputId": "49004e92-4b8d-4a53-97fb-8b5731b5a868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR=20\n",
            ".\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|                                                                                       | 0/367 [00:00<?, ?batch/s]C:\\Users\\PC\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:968.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
            "100%|██████████████████████████████████████████████████| 367/367 [01:31<00:00,  4.02batch/s, loss=8.88, metric=7.16e+3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LR=15\n",
            ".\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████| 367/367 [01:26<00:00,  4.24batch/s, loss=6.75, metric=857]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LR=10\n",
            ".\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████| 367/367 [01:20<00:00,  4.56batch/s, loss=6.75, metric=858]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LR=7.5\n",
            ".\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████| 367/367 [01:28<00:00,  4.16batch/s, loss=6.79, metric=894]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LR=5\n",
            ".\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████| 367/367 [01:28<00:00,  4.16batch/s, loss=6.84, metric=939]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LR=2.5\n",
            ".\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▉                                                   | 7/367 [00:01<01:40,  3.59batch/s, loss=9.56, metric=1.42e+4]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwd, momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 16\u001b[0m   model, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
            "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, loss_fn, optimizer, metric, epoch)\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), targets\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39mclip)\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "\n",
        "for lr in [20, 15, 10, 7.5, 5, 2.5]:\n",
        "  print(f'LR={lr}')\n",
        "\n",
        "  model = LanguageModel(vocab_size=len(vocab), embedding_dim=embedding_dim,\n",
        "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
        "                      dropoute=dropoute, dropouti=dropouti,\n",
        "                      dropouth=dropouth, dropouto=dropouto,\n",
        "                      weight_drop=weight_drop, pretrained=True).to(device)\n",
        "  # model = torch.load('model.pt')\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model, _, _ = train_one_epoch(model, train_loader, loss_fn, optimizer, metric, epoch)\n",
        "\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC2GhaXfA8vC"
      },
      "source": [
        "### 🟡 Step 4: Create a small grid using the weight decay and the best learning rate.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a7UeNW3WWaPu",
        "outputId": "e51f3f4b-776d-4dc1-82d3-52aad3af3a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR=7, WD=1.2e-06\n",
            ".\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████| 367/367 [01:28<00:00,  4.15batch/s, loss=6.68, metric=795]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LR=8, WD=1.2e-06\n",
            ".\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████| 367/367 [01:29<00:00,  4.11batch/s, loss=6.61, metric=745]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LR=14, WD=1.2e-06\n",
            ".\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n",
            "Applying weight drop of 0.5 to weight_hh_l0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|█▌                                                 | 11/367 [00:02<01:32,  3.83batch/s, loss=11.3, metric=8.28e+4]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwd, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 16\u001b[0m   model, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
            "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, loss_fn, optimizer, metric, epoch)\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), targets\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39mclip)\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\howsam-deep\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "\n",
        "for lr in [7, 8, 14, 13, 12, 11, 10, 9]:\n",
        "  for wd in [1.2e-6]:\n",
        "    print(f'LR={lr}, WD={wd}')\n",
        "\n",
        "    model = LanguageModel(vocab_size=len(vocab), embedding_dim=embedding_dim,\n",
        "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
        "                      dropoute=dropoute, dropouti=dropouti,\n",
        "                      dropouth=dropouth, dropouto=dropouto,\n",
        "                      weight_drop=weight_drop, pretrained=True).to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      model, _, _ = train_one_epoch(model, train_loader, loss_fn, optimizer, metric, epoch)\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjd9Z3N1ef3I"
      },
      "source": [
        "### 🟡 Step 5: Train model for longer epochs using the best model from step 4.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWgkMgC6JWpU"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(len(vocab), embedding_dim=300,\n",
        "                      hidden_dim=512, num_layers=2,\n",
        "                      dropout_embd=0.5, dropout_rnn=0.2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go-HitNQV_R1"
      },
      "outputs": [],
      "source": [
        "model = torch.load('/content/model-ppl_133.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVwLp-02JWpV"
      },
      "outputs": [],
      "source": [
        "lr = 3\n",
        "wd = 1e-6\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqxSVVB7JWpW"
      },
      "outputs": [],
      "source": [
        "loss_train_hist = []\n",
        "loss_valid_hist = []\n",
        "\n",
        "metric_train_hist = []\n",
        "metric_valid_hist = []\n",
        "\n",
        "best_loss_valid = torch.inf\n",
        "epoch_counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVqS9SEPJWpW"
      },
      "outputs": [],
      "source": [
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  # WandB\n",
        "  run = wandb.init(\n",
        "        project=\"language-modeling-lstms\",\n",
        "        config={\n",
        "            \"learning_rate\": lr,\n",
        "            \"epochs\": num_epochs,\n",
        "        })\n",
        "\n",
        "  # Train\n",
        "  model, loss_train, metric_train = train_one_epoch(model,\n",
        "                                                    train_loader,\n",
        "                                                    loss_fn,\n",
        "                                                    optimizer,\n",
        "                                                    metric,\n",
        "                                                    epoch)\n",
        "  # Validation\n",
        "  loss_valid, metric_valid = evaluate(model,\n",
        "                                      valid_loader,\n",
        "                                      loss_fn,\n",
        "                                      metric)\n",
        "\n",
        "  loss_train_hist.append(loss_train)\n",
        "  loss_valid_hist.append(loss_valid)\n",
        "\n",
        "  metric_train_hist.append(metric_train)\n",
        "  metric_valid_hist.append(metric_valid)\n",
        "\n",
        "  if loss_valid < best_loss_valid:\n",
        "    torch.save(model, f'model.pt')\n",
        "    best_loss_valid = loss_valid\n",
        "    print('Model Saved!')\n",
        "\n",
        "  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n",
        "  print()\n",
        "\n",
        "  epoch_counter += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjGQ-M02cusP"
      },
      "source": [
        "## 🟠 Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsWyc30h3mef"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGRWkn5wm9oP"
      },
      "source": [
        "🔰 Define train dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF3H9DeD6TCn"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8cmqYrL6UzZ"
      },
      "source": [
        "🔰 Define model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCtZXDybxexf",
        "outputId": "3f1837b2-2028-44f9-e381-fd9e9dffd582"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (embedding): Embedding(28782, 400)\n",
              "  (lstms): ModuleList(\n",
              "    (0): LSTM(400, 1150)\n",
              "    (1): LSTM(1150, 1150)\n",
              "    (2): LSTM(1150, 400)\n",
              "  )\n",
              "  (fc): Linear(in_features=400, out_features=28782, bias=True)\n",
              "  (lockdrop): LockedDropout()\n",
              ")"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "model = LanguageModel(vocab_size=len(vocab), embedding_dim=embedding_dim,\n",
        "                      hidden_dim=hidden_dim, num_layers=num_layers,\n",
        "                      dropoute=dropoute, dropouti=dropouti,\n",
        "                      dropouth=dropouth, dropouto=dropouto,\n",
        "                      weight_drop=weight_drop, pretrained=pretrained).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klf6g_N8c-Ub"
      },
      "outputs": [],
      "source": [
        "# model = torch.load('model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUKZRiQPxqrB"
      },
      "source": [
        "🔰 Define optimizer and Set learning rate and weight decay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bowjVB5yIXUP",
        "outputId": "2d5de7a9-274f-4ba9-8856-36fb255c5405"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    foreach: None\n",
              "    lr: 7.5\n",
              "    maximize: False\n",
              "    momentum: 0.9\n",
              "    nesterov: False\n",
              "    weight_decay: 1.2e-06\n",
              ")"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "lr = 7.5\n",
        "# wd = 1e-6\n",
        "# momentum = 0.9\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "# optimizer = optim.SGD([{'params': model.embedding.parameters(), 'lr': 0.1*lr},\n",
        "#                        {'params': model.lstms.parameters(), 'lr': lr}],\n",
        "#                       weight_decay=wd, momentum=momentum)\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AdYaMU4x34g"
      },
      "source": [
        "🔰 Initialize `wandb`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yboUzafnGD8"
      },
      "outputs": [],
      "source": [
        "if wandb_enable:\n",
        "  wandb.init(\n",
        "      project='LM-AWD-LSTM',\n",
        "      name=wandb_arg_name,\n",
        "      config={\n",
        "          'lr': lr,\n",
        "          'momentum': momentum,\n",
        "          'batch_size': batch_size,\n",
        "          'seq_len': seq_len,\n",
        "          'hidden_dim': hidden_dim,\n",
        "          'embedding_dim': embedding_dim,\n",
        "          'num_layers': num_layers,\n",
        "          'dropout_embed': dropoute,\n",
        "          'dropout_in_lstm': dropouti,\n",
        "          'dropout_h_lstm': dropouth,\n",
        "          'dropout_out_lstm': dropouto,\n",
        "          'clip': clip,\n",
        "      }\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUyFFIzlyaiB"
      },
      "source": [
        "🔰 Write code to train the model for `num_epochs` epoches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAXagB4yvtZd"
      },
      "outputs": [],
      "source": [
        "loss_train_hist = []\n",
        "loss_valid_hist = []\n",
        "\n",
        "metric_train_hist = []\n",
        "metric_valid_hist = []\n",
        "\n",
        "best_loss_valid = torch.inf\n",
        "epoch_counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PovABWnU3ld0",
        "outputId": "207b76c5-1d12-4f63-f5d1-77bda60217b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|█████████████████████████████████████████████| 733/733 [02:29<00:00,  4.91batch/s, loss=6.31, metric=552]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 5.503, Metric = 245.9\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|█████████████████████████████████████████████| 733/733 [02:29<00:00,  4.89batch/s, loss=5.61, metric=275]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 5.132, Metric = 169.7\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.88batch/s, loss=5.33, metric=207]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.936, Metric = 139.4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=5.16, metric=173]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.838, Metric = 126.5\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.88batch/s, loss=5.04, metric=155]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.805, Metric = 122.3\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.88batch/s, loss=4.95, metric=141]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.726, Metric = 113.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.88, metric=132]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.693, Metric = 109.3\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|█████████████████████████████████████████████| 733/733 [02:31<00:00,  4.83batch/s, loss=4.82, metric=124]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.678, Metric = 107.6\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.77, metric=118]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.662, Metric = 105.9\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.73, metric=113]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.637, Metric = 103.3\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|█████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.7, metric=109]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid: Loss = 4.651, Metric = 104.9\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.67, metric=106]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.617, Metric = 101.3\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.64, metric=104]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.605, Metric = 100.1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.61, metric=101]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.6, Metric = 99.56\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|███████████████████████████████████████████| 733/733 [02:30<00:00,  4.88batch/s, loss=4.59, metric=98.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.578, Metric = 97.46\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|███████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.57, metric=96.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid: Loss = 4.59, Metric = 98.56\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|███████████████████████████████████████████| 733/733 [02:30<00:00,  4.87batch/s, loss=4.56, metric=95.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.574, Metric = 97.03\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|███████████████████████████████████████████| 733/733 [02:29<00:00,  4.90batch/s, loss=4.54, metric=93.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 4.566, Metric = 96.32\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|███████████████████████████████████████████| 733/733 [02:29<00:00,  4.91batch/s, loss=4.52, metric=92.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid: Loss = 4.573, Metric = 97.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|█████████████████████████████████████████████| 733/733 [02:29<00:00,  4.91batch/s, loss=4.51, metric=91]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid: Loss = 4.568, Metric = 96.51\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|████████████████████████████████████████████| 733/733 [02:30<00:00,  4.86batch/s, loss=4.5, metric=90.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid: Loss = 4.568, Metric = 96.5\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22:  10%|████▌                                       | 75/733 [00:15<02:19,  4.73batch/s, loss=4.46, metric=86.9]"
          ]
        }
      ],
      "source": [
        "set_seed(seed)\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  # Train\n",
        "  model, loss_train, metric_train = train_one_epoch(model,\n",
        "                                                    train_loader,\n",
        "                                                    loss_fn,\n",
        "                                                    optimizer,\n",
        "                                                    metric,\n",
        "                                                    epoch)\n",
        "  # Validation\n",
        "  loss_valid, metric_valid = evaluate(model,\n",
        "                                      valid_loader,\n",
        "                                      loss_fn,\n",
        "                                      metric)\n",
        "\n",
        "  loss_train_hist.append(loss_train)\n",
        "  loss_valid_hist.append(loss_valid)\n",
        "\n",
        "  metric_train_hist.append(metric_train)\n",
        "  metric_valid_hist.append(metric_valid)\n",
        "\n",
        "  if loss_valid < best_loss_valid:\n",
        "    torch.save(model, f'model.pt')\n",
        "    best_loss_valid = loss_valid\n",
        "    print('Model Saved!')\n",
        "\n",
        "  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n",
        "  print()\n",
        "\n",
        "  if wandb_enable:\n",
        "    wandb.log({\"metric_train\": metric_train, \"loss_train\": loss_train,\n",
        "                \"metric_valid\": metric_valid, \"loss_valid\": loss_valid})\n",
        "\n",
        "  epoch_counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDKmOvCY2AYX"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK20iNRI3Xxb"
      },
      "source": [
        "## 🟠 Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKlLvCwuzEAA"
      },
      "source": [
        "🔰 Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYFzTsdIOkVp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(range(epoch_counter), loss_train_hist, 'r-', label='Train')\n",
        "plt.plot(range(epoch_counter), loss_valid_hist, 'b-', label='Validation')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ9UIdmkfxlA"
      },
      "source": [
        "# 🔴 **Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO8iPWH1zVYn"
      },
      "source": [
        "🔰 Test your model using data from the test set and images that are not present in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09Q1Cwaa6sGb"
      },
      "outputs": [],
      "source": [
        "model_path = 'model.pt'\n",
        "model = torch.load(model_path)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dgtH46XBWPF"
      },
      "outputs": [],
      "source": [
        "loss_valid, metric_valid = evaluate(model, valid_loader, loss_fn, metric)\n",
        "metric_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35sn67IhKcm_"
      },
      "outputs": [],
      "source": [
        "loss_test, metric_test = evaluate(model, test_loader, loss_fn, metric)\n",
        "metric_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzcQQwFuar_7"
      },
      "source": [
        "# 🔴 **Generate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh2_9jUp0GF4"
      },
      "source": [
        "🔰 Your mission is to write a `generate` function and use a desired sentence to evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pskvb--R-wJ0"
      },
      "outputs": [],
      "source": [
        "model_path = 'model.pt'\n",
        "model = torch.load(model_path)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsTQu0p6RsgO"
      },
      "outputs": [],
      "source": [
        "prompt = 'In a galaxy far, far away, there'\n",
        "\n",
        "indices = vocab(tokenizer(prompt))\n",
        "itos = vocab.get_itos()\n",
        "\n",
        "max_seq_len = 35\n",
        "for i in range(max_seq_len):\n",
        "  src = torch.LongTensor(indices).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    prediction = model(src)\n",
        "\n",
        "  # Method 1\n",
        "  # idx = torch.argmax(prediction[-1])\n",
        "  # itos = vocab.get_itos()\n",
        "  # itos[idx]\n",
        "\n",
        "  # Method 2\n",
        "  temperature = 0.5\n",
        "  probs = torch.softmax(prediction[-1]/temperature, dim=0)\n",
        "\n",
        "  idx = vocab['<ukn>']\n",
        "  while idx == vocab['<ukn>']:\n",
        "    idx = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "  token = itos[idx]\n",
        "  prompt += ' ' + token\n",
        "\n",
        "  if idx == vocab['.']:\n",
        "    break\n",
        "\n",
        "  indices.append(idx)\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5SvSDLal8YB"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, seed=None):\n",
        "  if seed is not None:\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "  indices = vocab(tokenizer(prompt))\n",
        "  itos = vocab.get_itos()\n",
        "\n",
        "  for i in range(max_seq_len):\n",
        "    src = torch.LongTensor(indices).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = model(src)\n",
        "\n",
        "    # Method 1\n",
        "    # idx = torch.argmax(prediction[-1])\n",
        "    # itos = vocab.get_itos()\n",
        "    # itos[idx]\n",
        "\n",
        "    # Method 2\n",
        "    probs = torch.softmax(prediction[-1]/temperature, dim=0)\n",
        "\n",
        "    idx = vocab['<ukn>']\n",
        "    while idx == vocab['<ukn>']:\n",
        "      idx = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "    token = itos[idx]\n",
        "    prompt += ' ' + token\n",
        "\n",
        "    if idx == vocab['.']:\n",
        "      return prompt\n",
        "\n",
        "    indices.append(idx)\n",
        "\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Cw2bzfRmY9"
      },
      "outputs": [],
      "source": [
        "prompt = 'In a galaxy far, far away, there'\n",
        "prompt = 'The sun was setting in the'\n",
        "prompt = 'Once upon a time, there lived a young princess named'\n",
        "prompt = 'What is the meaning '\n",
        "\n",
        "generate(prompt, 35, 0.5, model, tokenizer, vocab)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VrFCH9hpEyip",
        "UcNIGF4CUCk3",
        "AFHw176FhqQa",
        "uWoVZ_MuKif0",
        "t6tRkdc5HoZT",
        "BNJRZe4QBudV",
        "RwaY_YcgRayy",
        "pwlVLNJXfUJw",
        "3in1e9BksgIh",
        "RTql4Ftiunfr",
        "ujIVtjsYvxOI",
        "wCi-ofSLCzop",
        "idRexFij4wgN",
        "PgLgP04P4-aX",
        "NCQjacybOfqV",
        "3ttl0AK3Hvyh",
        "24qT-sgUO2-d",
        "W0QNbC0YPCKZ",
        "G9HgVWslPGsH",
        "o_5f69nwPtY2",
        "De7VreNxQdct",
        "lpJ3wtyctQJH",
        "BrHQCv7q7LF_",
        "BLT4w0ZfAhlJ",
        "uC2GhaXfA8vC",
        "Mjd9Z3N1ef3I",
        "rjGQ-M02cusP",
        "oK20iNRI3Xxb",
        "KZ9UIdmkfxlA",
        "FzcQQwFuar_7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}